{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torchmetrics import MeanSquaredLogError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import utils.load_data as ld\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from skimage.feature import blob_dog\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"setting random seeds\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DECORAS_Enc2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_Enc2D, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Linear(64 * 16 * 16, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "class DECORAS_Dec2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_Dec2D, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(256,  64  * 16 * 16)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(64, 16, 16))\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32))\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16))\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        self.out = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=(1, 1), bias=False, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        #print(x.size())\n",
    "        x = self.unflatten(x)\n",
    "        #print(x.size())\n",
    "        x = self.block1(x)\n",
    "        #print(x.size())\n",
    "        x = self.block2(x)\n",
    "        #print(x.size())\n",
    "        x = self.block3(x)\n",
    "        #print(x.size())\n",
    "        x = self.block4(x)\n",
    "        #print(x.size())\n",
    "        x = self.out(x)\n",
    "        #print(x.size())\n",
    "        return x\n",
    "            \n",
    "class DECORAS_BF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_BF, self).__init__()\n",
    "    \n",
    "        self.enc = DECORAS_Enc2D()\n",
    "        self.dec = DECORAS_Dec2D()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        lat = self.enc(x)\n",
    "        out = self.dec(lat)\n",
    "        return out\n",
    "\n",
    "class DECORAS_Enc2DSS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_Enc2DSS, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), bias=False, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        self.dense = nn.Linear(32 * 16 * 16, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "class DECORAS_Dec2DSS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_Dec2DSS, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(64,  32  * 16 * 16)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 16, 16))\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16))\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8))\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=(3, 3), bias=False, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        self.out = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=(1, 1), bias=False, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        #print(x.size())\n",
    "        x = self.unflatten(x)\n",
    "        #print(x.size())\n",
    "        x = self.block1(x)\n",
    "        #print(x.size())\n",
    "        x = self.block2(x)\n",
    "        #print(x.size())\n",
    "        x = self.block3(x)\n",
    "        #print(x.size())\n",
    "        x = self.out(x)\n",
    "        #print(x.size())\n",
    "        return x\n",
    "\n",
    "class DECORAS_BF2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DECORAS_BF2, self).__init__()\n",
    "    \n",
    "        self.enc = DECORAS_Enc2DSS()\n",
    "        self.dec = DECORAS_Dec2DSS()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        lat = self.enc(x)\n",
    "        out = self.dec(lat)\n",
    "        return out\n",
    "\n",
    "def save_checkpoint(model, optimizer, save_path, epoch):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "        save_path,\n",
    "    )\n",
    "\n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    return model, optimizer, epoch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, save_path, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            save_checkpoint(val_loss, model, optimizer, save_path, epoch)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            save_checkpoint(val_loss, model, optimizer, save_path, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "def train_batch(inputs, targets, model, optimizer, criterion):\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(criterion, list):\n",
    "        loss = 0\n",
    "        for i in range(len(criterion)):\n",
    "            loss += criterion[i](outputs, targets)\n",
    "    else:\n",
    "        loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, outputs\n",
    "\n",
    "def valid_batch(inputs, targets, model, optimizer, criterion):\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(criterion, list):\n",
    "        loss = 0\n",
    "        for i in range(len(criterion)):\n",
    "            loss += criterion[i](outputs, targets)\n",
    "    else:\n",
    "        loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    return loss, outputs\n",
    "\n",
    "def test_batch(inputs, targets, model, criterion):\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(criterion, list):\n",
    "        loss = 0\n",
    "        for i in range(len(criterion)):\n",
    "            loss += criterion[i](outputs, targets)\n",
    "    else:\n",
    "        loss = criterion(outputs, targets)\n",
    "    return loss, outputs\n",
    "\n",
    "def train_log(loss, optimizer, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": loss, 'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "\n",
    "def valid_log(loss):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"valid_loss\": loss})\n",
    "\n",
    "def test_log(loss):\n",
    "    wandb.log({\"test_loss\": loss})\n",
    "\n",
    "def log_images(inputs, predictions, targets, mode='Train'):\n",
    "    idxs  = random.sample(list(np.arange(0, len(predictions))), 8)\n",
    "    inputs_log = inputs[idxs]\n",
    "    predictions_log = predictions[idxs]\n",
    "    targets_log = targets[idxs]\n",
    "    images = torch.cat((inputs_log, predictions_log, targets_log), dim=0)\n",
    "    images = make_grid(images, nrow=8,  normalize=True)\n",
    "    imgs = wandb.Image(images, caption=\"Top: Inputs, Center: Predictions, Bottom: Targets\")\n",
    "    if mode == 'Train':\n",
    "        wandb.log({\"train_examples\": imgs})\n",
    "    elif mode == 'Validation':\n",
    "        wandb.log({\"validation_examples\": imgs})\n",
    "    else:\n",
    "        wandb.log({\"test_examples\": imgs})\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, config, name, device):\n",
    "    example_ct = 0\n",
    "    best_loss = 9999\n",
    "    # initialize the early_stopping object\n",
    "    if config['early_stopping']:\n",
    "        early_stopping = EarlyStopping(patience=config['patience'], verbose=False)\n",
    "    outpath = os.sep.join((config['output_dir'], name + \".pt\"))\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i_batch, batch in tqdm(enumerate(train_loader)):\n",
    "            inputs = batch[0].to(device)\n",
    "            targets = batch[1].to(device)\n",
    "            loss, outputs = train_batch(inputs, targets, model, optimizer, criterion)\n",
    "            example_ct += len(inputs)\n",
    "            train_log(loss, optimizer, epoch)\n",
    "            if i_batch == len(train_loader) - 1:\n",
    "                log_images(inputs, outputs, targets, 'Train')\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Training Loss {epoch_loss}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        valid_losses = []\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in tqdm(enumerate(valid_loader)):\n",
    "                inputs = batch[0].to(device)\n",
    "                targets = batch[1].to(device)\n",
    "                loss, outputs = valid_batch(inputs, targets, model, optimizer, criterion)\n",
    "                valid_log(loss)\n",
    "                if i_batch == len(valid_loader) - 1:\n",
    "                    log_images(inputs, outputs, targets, 'Validation')\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                valid_losses.append(loss.item())\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "        print(f\"Validation Loss {epoch_loss}\")\n",
    "        if config['early_stopping']:\n",
    "            early_stopping(valid_loss, model, optimizer, outpath, epoch)\n",
    "        else:\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                save_checkpoint(model, optimizer, outpath , epoch)\n",
    "\n",
    "    model, _, _ = load_checkpoint(model, optimizer, outpath)\n",
    "    return model\n",
    "\n",
    "def test(model1, model2, test_loader, criterion1, criterion2, config, device):\n",
    "    model1.eval()\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    if not os.path.exists(config['plot_dir']):\n",
    "        os.mkdir(config['plot_dir'])\n",
    "    if not os.path.exists(config['prediction_dir']):\n",
    "        os.mkdir(config['prediction_dir'])\n",
    "    true_x = []\n",
    "    true_y = []\n",
    "    predicted_x = []\n",
    "    predicted_y = []\n",
    "    for i_batch, batch in tqdm(enumerate(test_loader)):\n",
    "        inputs = batch[0].to(device)\n",
    "        targets = batch[1].to(device)\n",
    "        target_boxes =  batch[2]\n",
    "        loss, outputs1 = test_batch(inputs, targets, model1, criterion1)\n",
    "        loss, outputs2 = test_batch(inputs, targets, model2, criterion2)\n",
    "        for b in tqdm(range(len(targets))):\n",
    "            output1 = outputs1[b, 0].cpu().detach().numpy()\n",
    "            min_, max_ = np.min(output1), np.max(output1)\n",
    "            output1 = (output1 - min_) / (max_ - min_)\n",
    "            tboxes = target_boxes[b]\n",
    "            blobs1 = blob_dog(output1, min_sigma=3)\n",
    "            pxs1 = blobs1[:, 0]\n",
    "            pys1 = blobs1[:, 1]\n",
    "            radiuses = np.sqrt(2 * blobs1[:, 2])\n",
    "            output2 = outputs2[b, 0].cpu().detach().numpy()\n",
    "            min_, max_ = np.min(output2), np.max(output2)\n",
    "            output2 = (output2 - min_) / (max_ - min_)\n",
    "            tboxes = target_boxes[b]\n",
    "            blobs2 = blob_dog(output2, min_sigma=3)\n",
    "            pxs2 = blobs2[:, 0]\n",
    "            pys2 = blobs2[:, 1]\n",
    "            tpxs, tpys = [], []\n",
    "            idxs = []\n",
    "            # Selecting only blobs which are predicted by both models\n",
    "            for j in range(len(pxs1)):\n",
    "                px1 = pxs1[j]\n",
    "                py1 = pys1[j]\n",
    "                k = 0\n",
    "                for k in range(len(pxs2)):\n",
    "                    dist = np.sqrt((px1 - pxs2[k])**2 + (py1 - pys2[k])**2)\n",
    "                    if dist <= 3 and k < 1:\n",
    "                        tpxs.append(px1)\n",
    "                        tpys.append(py1)\n",
    "                        idxs.append(j)\n",
    "            tpxs = np.array(tpxs)\n",
    "            tpys = np.array(tpys)\n",
    "            idxs = np.array(idxs)\n",
    "            radiuses = radiuses[idxs]\n",
    "            boxes = []\n",
    "            # measuing boundinb boxes from radiuses and centers\n",
    "            for j in range(len(radiuses)):\n",
    "                y0 = tpys[j] - radiuses[j]\n",
    "                y1 = tpys[j] + radiuses[j]\n",
    "                x0 = tpxs[j] - radiuses[j]\n",
    "                x1 = tpxs[j] + radiuses[j]\n",
    "                boxes.append([y0, x0, y1, x1])\n",
    "            boxes = np.array(boxes)\n",
    "            txc = tboxes[:, 1] + 0.5 * (tboxes[:, 3] - tboxes[:, 1])\n",
    "            tyc = tboxes[:, 0] + 0.5 * (tboxes[:, 2] - tboxes[:, 0])\n",
    "            # merasuring distances and IoUs between true and predicted bounding boxes\n",
    "            dists = []\n",
    "            for j in range(len(txc)):\n",
    "                d = []\n",
    "                for k in range(len(tpxs)):\n",
    "                    d.append(np.sqrt((txc[j] - tpxs[k])**2 + (tyc[j] - tpys[k])**2))\n",
    "                    dists.append(d)\n",
    "            dists = np.array(dists)\n",
    "            idxs = np.argmin(dists, axis=1)\n",
    "            dists = np.min(dists, axis=1)\n",
    "            ious = box_iou(torch.Tensor(tboxes), torch.Tensor(boxes)).numpy()\n",
    "            ious = np.max(ious, axis=1)\n",
    "            for i in range(len(dists)):\n",
    "                if ious[i] >= config['twoD_iou_threshold'] and dists[i] <= config['twoD_dist_threshold']:\n",
    "                    true_x.append(txc[i])\n",
    "                    true_y.append(tyc[i])\n",
    "                    predicted_x.append(tpxs[idxs[i]])\n",
    "                    predicted_y.append(tpys[idxs[i]])\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            if len(boxes) > len(tboxes):\n",
    "                fp += len(boxes) - len(tboxes)\n",
    "    \n",
    "    true_x = np.array(true_x)\n",
    "    true_y = np.array(true_y)\n",
    "    predicted_x = np.array(predicted_x)\n",
    "    predicted_y = np.array(predicted_y)\n",
    "    decoras_x_predictions_name = os.path.join(config['prediction_dir'], 'decoras_x_predictions.npy')\n",
    "    decoras_y_predictions_name = os.path.join(config['prediction_dir'], 'decoras_y_predictions.npy')\n",
    "    decoras_x_true_name = os.path.join(config['prediction_dir'], 'decoras_x_true.npy')\n",
    "    decoras_y_true_name = os.path.join(config['prediction_dir'], 'decoras_y_true.npy')\n",
    "    np.save(decoras_x_predictions_name, predicted_x)\n",
    "    np.save(decoras_y_predictions_name, predicted_y)\n",
    "    np.save(decoras_x_true_name, true_x)\n",
    "    np.save(decoras_y_true_name, true_y)\n",
    "    names = ['tp', 'fp', 'fn']\n",
    "    values = [tp, fp, fn]\n",
    "    rdb = pd.DataFrame(data=values, columns=names)\n",
    "    result_name = os.path.join(config['prediction_dir'], 'decoras_results.csv')\n",
    "    rdb.to_csv(result_name)\n",
    "    return tp, len(test_loader.dataset), fp, fn, true_x, true_y, predicted_x, predicted_y\n",
    "\n",
    "def make(config, device):\n",
    "    output_dir = config['output_dir']\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    train_dir = config['data_folder'] + 'Train/'\n",
    "    valid_dir = config['data_folder'] + 'Validation/'\n",
    "    test_dir = config['data_folder'] + 'Test/'\n",
    "    crop = ld.Crop(256)\n",
    "    rotate = ld.RandomRotate()\n",
    "    hflip = ld.RandomHorizontalFlip(p=1)\n",
    "    vflip = ld.RandomVerticalFlip(p=1)\n",
    "    norm_img = ld.NormalizeImage()\n",
    "    to_tensor = ld.ToTensor()\n",
    "    train_compose = transforms.Compose([rotate, vflip, hflip, crop, norm_img, to_tensor])\n",
    "    if config['mode'] == 'train':\n",
    "        print('Preparing Data for Blobs Finder Training and Testing...')\n",
    "        train_dataset = ld.ALMADataset('train_params.csv', train_dir, transform=train_compose)\n",
    "        valid_dataset = ld.ALMADataset('valid_params.csv', valid_dir, transform=train_compose)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], num_workers=os.cpu_count(), \n",
    "                          pin_memory=True, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], num_workers=os.cpu_count(), \n",
    "                          pin_memory=True, shuffle=True, collate_fn=valid_dataset.collate_fn)\n",
    "    else:\n",
    "        print(\"Preparing Data for Blobs Finder Testing....\")\n",
    "    test_compose = transforms.Compose([crop, norm_img, to_tensor])\n",
    "    test_dataset = ld.ALMADataset('test_params.csv', test_dir, transform=test_compose)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], num_workers=os.cpu_count(), \n",
    "                          pin_memory=True, shuffle=True, collate_fn=test_dataset.collate_fn)\n",
    "    model1 = DECORAS_BF()\n",
    "    model2 = DECORAS_BF()\n",
    "    if torch.cuda.device_count() > 1 and config['multi_gpu']:\n",
    "        print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "        model1 = nn.DataParallel(model1)\n",
    "        model2 = nn.DataParallel(model2)\n",
    "    print(f'Using {device}') \n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "    criterion1 = MeanSquaredLogError()\n",
    "    criterion2 = nn.BCELoss()\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=config['learning_rate'], \n",
    "                                         weight_decay=config['weight_decay'])\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=config['learning_rate'], \n",
    "                                         weight_decay=config['weight_decay'])\n",
    "    if config['model'] == 'train':\n",
    "        return model1, model2, criterion1, criterion2, optimizer1, optimizer2, train_loader, valid_loader\n",
    "    else:\n",
    "        print('Loading Checkpoint....')\n",
    "        outpath1 = os.sep.join((config['output_dir'], config['model1_name'] + \".pt\"))\n",
    "        outpath2 = os.sep.join((config['output_dir'], config['model2_name'] + \".pt\"))\n",
    "        model1, _, _ = load_checkpoint(model1, optimizer1, outpath1)\n",
    "        model2, _, _ = load_checkpoint(model2, optimizer2, outpath2)\n",
    "        return model1, model2, criterion1, criterion2, optimizer1, optimizer2, test_loader\n",
    "\n",
    "def decoras_train_model(hyperparameters, device, name):\n",
    "    with wandb.init(project='hyperparameters', name=name, entity='bradipo', config=hyperparameters):\n",
    "        config = wandb.config\n",
    "        bf_msle, bf_bce, msle, bce, optimizer1, optimizer2, train_loader, valid_loader = make(config, device)\n",
    "        if name == 'msle_decoras':\n",
    "            train(bf_msle, train_loader, valid_loader, msle, optimizer1, config, name, device)\n",
    "        else:\n",
    "            train(bf_bce, train_loader, valid_loader, bce, optimizer1, config, name, device)\n",
    "\n",
    "def decoras_test(hyperparameters, device, name):\n",
    "    with wandb.init(project='hyperparameters', name=name, entity='bradipo', config=hyperparameters):\n",
    "        config = wandb.config\n",
    "        bf_msle, bf_bce, msle, bce, optimizer1, optimizer2, test_loader = make(config, device)\n",
    "        tp, n, fp, fn, true_x, true_y, predicted_x, predicted_y = test(bf_msle, bf_bce, msle, bce, optimizer1,\n",
    "                                                                        optimizer2, config, device)\n",
    "    return tp, n, fp, fn, true_x, true_y, predicted_x, predicted_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs = 200,\n",
    "    batch_size = 64,\n",
    "    multi_gpu = False,\n",
    "    mode = 'train',\n",
    "    learning_rate = 0.001,\n",
    "    weight_decay = 1e-5,\n",
    "    early_stopping = 'True',\n",
    "    patience = 20, \n",
    "    warm_start=False,\n",
    "    warm_start_iterations = 3,\n",
    "    twoD_iou_threshold = 0.8,\n",
    "    twoD_dist_threshold = 3, \n",
    "    data_folder='/lustre/home/mdelliveneri/ALMADL/data/',\n",
    "    project = 'Decoras',\n",
    "    output_dir = '/lustre/home/mdelliveneri/ALMADL/trained_models',\n",
    "    prediction_dir = '/lustre/home/mdelliveneri/ALMADL/predictions',\n",
    "    plot_dir = '/lustre/home/mdelliveneri/ALMADL/plots',\n",
    "    model1_name = 'msle_decoras',\n",
    "    model2_name = 'bce_decoras',\n",
    ")\n",
    "# Device configuration, later modify to work with multiple GPUs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoras_train_model(config, device, config['model1_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
